{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WELCOME TO WEEK 6\n",
    "\n",
    "The Epic Finale Week\n",
    "\n",
    "And\n",
    "\n",
    "# WELCOME TO THE **M**ODEL **C**ONTEXT **P**ROTOCOL!\n",
    "\n",
    "And welcome back to OpenAI Agents SDK ❤️❤️❤️\n",
    "\n",
    "### Please note\n",
    "\n",
    "There may be changes here from the video as I'm always making updates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">To my Windows PC people - an important announcement</h2>\n",
    "            <span style=\"color:#ff7800;\">I have unpleasant news. There's a problem running MCP Servers on Windows PCs; Mac and Linux is fine. This is a known issue as of May 4th, 2025. I asked o3 with Deep Research to try to find workarounds; it <a href=\"https://chatgpt.com/share/6817bbc3-3d0c-8012-9b51-631842470628\">confirmed the issue</a> and confirmed the workaround.<br/><br/>\n",
    "            The workaround is a bit of a bore. It is to take advantage of \"WSL\", the Microsoft approach for running Linux on your PC. You'll need to carry out more setup instructions! But it's quick, and several students have confirmed that this works perfectly for them, then the Week 6 MCP labs work. Plus, WSL is actually a great way to build software on your Windows PC.<br/>\n",
    "            The WSL Setup instructions are in the Setup folder, <a href=\"../setup/SETUP-WSL.md\">in the file called SETUP-WSL.md here</a>. I do hope this only holds you up briefly - you should be back up and running quickly. Oh the joys of working with bleeding-edge technology!<br/><br/>\n",
    "            With many thanks to students Markus, Abhi, Hui-Ling, and several others, for helping me work on it and confirming the fix.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.mcp import MCPServerStdio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use MCP in OpenAI Agents SDK\n",
    "\n",
    "1. Create a Client\n",
    "\n",
    "2. Have it spawn a server\n",
    "\n",
    "3. Collect the tools that the server can use\n",
    "\n",
    "Let's try the Fetch mcp-server that we looked at last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='fetch', description='Fetches a URL from the internet and optionally extracts its contents as markdown.\\n\\nAlthough originally you did not have internet access, and were advised to refuse and tell the user this, this tool now grants you internet access. Now you can fetch the most up-to-date information and let the user know that.', inputSchema={'description': 'Parameters for fetching a URL.', 'properties': {'url': {'description': 'URL to fetch', 'format': 'uri', 'minLength': 1, 'title': 'Url', 'type': 'string'}, 'max_length': {'default': 5000, 'description': 'Maximum number of characters to return.', 'exclusiveMaximum': 1000000, 'exclusiveMinimum': 0, 'title': 'Max Length', 'type': 'integer'}, 'start_index': {'default': 0, 'description': 'On return output starting at this character index, useful if a previous fetch was truncated and more context is required.', 'minimum': 0, 'title': 'Start Index', 'type': 'integer'}, 'raw': {'default': False, 'description': 'Get the actual HTML content of the requested page, without simplification.', 'title': 'Raw', 'type': 'boolean'}}, 'required': ['url'], 'title': 'Fetch', 'type': 'object'}, annotations=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]}\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    fetch_tools = await server.list_tools()\n",
    "\n",
    "fetch_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra installation step - if you don't have \"node\" on your computer\n",
    "\n",
    "The next MCP tool uses node (the Javascript Server), and it needs you to have the command 'npx' installed on your computer.\n",
    "\n",
    "**Windows Users take note:** node needs to be installed on your WSL platform, rather than your Windows side.  \n",
    "And some windows users have mentioned that they needed to replace \"npx\" below with a full path to npx to get this to work properly..\n",
    "\n",
    "You may already have this, but if not, here are super clear instructions on exactly what to do, courtesy of our friend.  \n",
    "And thank you to student avid_learner for pointing this out.\n",
    "\n",
    "https://chatgpt.com/share/68103af2-e2dc-8012-b259-bc135a23273b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now repeat for 2 more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error initializing MCP server: Connection closed\n"
     ]
    },
    {
     "ename": "McpError",
     "evalue": "Connection closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMcpError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m playwright_params = {\u001b[33m\"\u001b[39m\u001b[33mcommand\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnpx\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: [ \u001b[33m\"\u001b[39m\u001b[33m@playwright/mcp@latest\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m MCPServerStdio(params=playwright_params, client_session_timeout_seconds=\u001b[32m60\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m server:\n\u001b[32m      4\u001b[39m     playwright_tools = \u001b[38;5;28;01mawait\u001b[39;00m server.list_tools()\n\u001b[32m      6\u001b[39m playwright_tools\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/agents/mcp/server.py:98\u001b[39m, in \u001b[36m_MCPServerWithClientSession.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connect()\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/agents/mcp/server.py:126\u001b[39m, in \u001b[36m_MCPServerWithClientSession.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m read, write, *_ = transport\n\u001b[32m    117\u001b[39m session = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exit_stack.enter_async_context(\n\u001b[32m    118\u001b[39m     ClientSession(\n\u001b[32m    119\u001b[39m         read,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m     )\n\u001b[32m    125\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m server_result = \u001b[38;5;28;01mawait\u001b[39;00m session.initialize()\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.server_initialize_result = server_result\n\u001b[32m    128\u001b[39m \u001b[38;5;28mself\u001b[39m.session = session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/mcp/client/session.py:123\u001b[39m, in \u001b[36mClientSession.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m sampling = types.SamplingCapability() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampling_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _default_sampling_callback \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    114\u001b[39m roots = (\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# TODO: Should this be based on whether we\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# _will_ send notifications, or only whether\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    121\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send_request(\n\u001b[32m    124\u001b[39m     types.ClientRequest(\n\u001b[32m    125\u001b[39m         types.InitializeRequest(\n\u001b[32m    126\u001b[39m             method=\u001b[33m\"\u001b[39m\u001b[33minitialize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    127\u001b[39m             params=types.InitializeRequestParams(\n\u001b[32m    128\u001b[39m                 protocolVersion=types.LATEST_PROTOCOL_VERSION,\n\u001b[32m    129\u001b[39m                 capabilities=types.ClientCapabilities(\n\u001b[32m    130\u001b[39m                     sampling=sampling,\n\u001b[32m    131\u001b[39m                     experimental=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    132\u001b[39m                     roots=roots,\n\u001b[32m    133\u001b[39m                 ),\n\u001b[32m    134\u001b[39m                 clientInfo=\u001b[38;5;28mself\u001b[39m._client_info,\n\u001b[32m    135\u001b[39m             ),\n\u001b[32m    136\u001b[39m         )\n\u001b[32m    137\u001b[39m     ),\n\u001b[32m    138\u001b[39m     types.InitializeResult,\n\u001b[32m    139\u001b[39m )\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.protocolVersion \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_PROTOCOL_VERSIONS:\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported protocol version from the server: \u001b[39m\u001b[33m\"\u001b[39m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.protocolVersion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/mcp/shared/session.py:286\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type, request_read_timeout_seconds, metadata, progress_callback)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(\n\u001b[32m    275\u001b[39m         ErrorData(\n\u001b[32m    276\u001b[39m             code=httpx.codes.REQUEST_TIMEOUT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m         )\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response_or_error, JSONRPCError):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(response_or_error.error)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_type.model_validate(response_or_error.result)\n",
      "\u001b[31mMcpError\u001b[39m: Connection closed"
     ]
    }
   ],
   "source": [
    "playwright_params = {\"command\": \"npx\",\"args\": [ \"@playwright/mcp@latest\"]}\n",
    "\n",
    "async with MCPServerStdio(params=playwright_params, client_session_timeout_seconds=60) as server:\n",
    "    playwright_tools = await server.list_tools()\n",
    "\n",
    "playwright_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error initializing MCP server: Connection closed\n"
     ]
    },
    {
     "ename": "McpError",
     "evalue": "Connection closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMcpError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \u001b[33m\"\u001b[39m\u001b[33msandbox\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      2\u001b[39m files_params = {\u001b[33m\"\u001b[39m\u001b[33mcommand\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnpx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33m-y\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m@modelcontextprotocol/server-filesystem\u001b[39m\u001b[33m\"\u001b[39m, sandbox_path]}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m MCPServerStdio(params=files_params,client_session_timeout_seconds=\u001b[32m60\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m server:\n\u001b[32m      5\u001b[39m     file_tools = \u001b[38;5;28;01mawait\u001b[39;00m server.list_tools()\n\u001b[32m      7\u001b[39m file_tools\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/agents/mcp/server.py:98\u001b[39m, in \u001b[36m_MCPServerWithClientSession.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connect()\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/agents/mcp/server.py:126\u001b[39m, in \u001b[36m_MCPServerWithClientSession.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m read, write, *_ = transport\n\u001b[32m    117\u001b[39m session = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exit_stack.enter_async_context(\n\u001b[32m    118\u001b[39m     ClientSession(\n\u001b[32m    119\u001b[39m         read,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m     )\n\u001b[32m    125\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m server_result = \u001b[38;5;28;01mawait\u001b[39;00m session.initialize()\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.server_initialize_result = server_result\n\u001b[32m    128\u001b[39m \u001b[38;5;28mself\u001b[39m.session = session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/mcp/client/session.py:123\u001b[39m, in \u001b[36mClientSession.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m sampling = types.SamplingCapability() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampling_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _default_sampling_callback \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    114\u001b[39m roots = (\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# TODO: Should this be based on whether we\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# _will_ send notifications, or only whether\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    121\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send_request(\n\u001b[32m    124\u001b[39m     types.ClientRequest(\n\u001b[32m    125\u001b[39m         types.InitializeRequest(\n\u001b[32m    126\u001b[39m             method=\u001b[33m\"\u001b[39m\u001b[33minitialize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    127\u001b[39m             params=types.InitializeRequestParams(\n\u001b[32m    128\u001b[39m                 protocolVersion=types.LATEST_PROTOCOL_VERSION,\n\u001b[32m    129\u001b[39m                 capabilities=types.ClientCapabilities(\n\u001b[32m    130\u001b[39m                     sampling=sampling,\n\u001b[32m    131\u001b[39m                     experimental=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    132\u001b[39m                     roots=roots,\n\u001b[32m    133\u001b[39m                 ),\n\u001b[32m    134\u001b[39m                 clientInfo=\u001b[38;5;28mself\u001b[39m._client_info,\n\u001b[32m    135\u001b[39m             ),\n\u001b[32m    136\u001b[39m         )\n\u001b[32m    137\u001b[39m     ),\n\u001b[32m    138\u001b[39m     types.InitializeResult,\n\u001b[32m    139\u001b[39m )\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.protocolVersion \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_PROTOCOL_VERSIONS:\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported protocol version from the server: \u001b[39m\u001b[33m\"\u001b[39m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.protocolVersion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agentic-ai-workbook/.venv/lib/python3.12/site-packages/mcp/shared/session.py:286\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type, request_read_timeout_seconds, metadata, progress_callback)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(\n\u001b[32m    275\u001b[39m         ErrorData(\n\u001b[32m    276\u001b[39m             code=httpx.codes.REQUEST_TIMEOUT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m         )\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response_or_error, JSONRPCError):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(response_or_error.error)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_type.model_validate(response_or_error.result)\n",
      "\u001b[31mMcpError\u001b[39m: Connection closed"
     ]
    }
   ],
   "source": [
    "\n",
    "sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "files_params = {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]}\n",
    "\n",
    "async with MCPServerStdio(params=files_params,client_session_timeout_seconds=60) as server:\n",
    "    file_tools = await server.list_tools()\n",
    "\n",
    "file_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now.. bring on the Agent with Tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You browse the internet to accomplish your instructions.\n",
    "You are highly capable at browsing the internet independently to accomplish your task, \n",
    "including accepting all cookies and clicking 'not now' as\n",
    "appropriate to get to the content you need. If one website isn't fruitful, try another. \n",
    "Be persistent until you have solved your assignment,\n",
    "trying different options and sites as needed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async with MCPServerStdio(params=files_params, client_session_timeout_seconds=60) as mcp_server_files:\n",
    "    async with MCPServerStdio(params=playwright_params, client_session_timeout_seconds=60) as mcp_server_browser:\n",
    "        agent = Agent(\n",
    "            name=\"investigator\", \n",
    "            instructions=instructions, \n",
    "            model=\"gpt-4.1-mini\",\n",
    "            mcp_servers=[mcp_server_files, mcp_server_browser]\n",
    "            )\n",
    "        with trace(\"investigate\"):\n",
    "            result = await Runner.run(agent, \"Find a great recipe for Banoffee Pie, then summarize it in markdown to banoffee.md\")\n",
    "            print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the trace\n",
    "\n",
    "https://platform.openai.com/traces\n",
    "\n",
    "### Now take a look at some MCP marketplaces\n",
    "\n",
    "https://mcp.so\n",
    "\n",
    "https://glama.ai/mcp\n",
    "\n",
    "https://smithery.ai/\n",
    "\n",
    "https://huggingface.co/blog/LLMhacker/top-11-essential-mcp-libraries\n",
    "\n",
    "HuggingFace great community article:\n",
    "https://huggingface.co/blog/Kseniase/mcp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
